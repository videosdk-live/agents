from __future__ import annotations

from abc import abstractmethod
from typing import Any, AsyncIterator, Literal, Optional
from pydantic import BaseModel
from ..event_emitter import EventEmitter
from .chat_context import ChatContext, ChatRole
from ..utils import FunctionTool


class LLMResponse(BaseModel):
    """
    Data model to hold LLM response data.

    This class represents the structured response from a Language Model (LLM),
    containing the generated content, role information, and optional metadata.
    It's used to standardize responses across different LLM providers.

    Attributes:
        content (str): The text content generated by the LLM.
        role (ChatRole): The role of the response (typically ASSISTANT).
        metadata (Optional[dict[str, Any]]): Additional response metadata from the LLM provider.
    """
    content: str
    role: ChatRole
    metadata: Optional[dict[str, Any]] = None


class LLM(EventEmitter[Literal["error"]]):
    """
    Base class for LLM implementations.

    This abstract base class defines the interface that all LLM providers
    must implement. It provides common functionality for event emission,
    resource management, and defines the contract for chat interactions.

    The class inherits from EventEmitter to support error event handling
    and can be used as an async context manager for proper resource cleanup.

    """

    def __init__(self) -> None:
        """
        Initialize the LLM base class.

        Sets up the internal label for identification and initializes
        the event emitter functionality.
        """
        super().__init__()
        self._label = f"{type(self).__module__}.{type(self).__name__}"

    @property
    def label(self) -> str:
        """
        Get the LLM provider label.

        Returns:
            str: A string identifier for the LLM provider (e.g., "videosdk.plugins.openai.llm.OpenAILLM").
        """
        return self._label

    @abstractmethod
    async def chat(
        self,
        messages: ChatContext,
        tools: list[FunctionTool] | None = None,
        **kwargs: Any
    ) -> AsyncIterator[LLMResponse]:
        """
        Main method to interact with the LLM.

        This abstract method must be implemented by all LLM providers. It handles
        the core chat interaction, processing messages and optionally using
        function tools, then yielding responses asynchronously.

        Args:
            messages (ChatContext): The conversation context containing message history.
            tools (list[FunctionTool] | None, optional): List of available function tools for the LLM to use.
            **kwargs (Any): Additional arguments specific to the LLM provider implementation.

        Returns:
            AsyncIterator[LLMResponse]: An async iterator yielding LLMResponse objects as they're generated.

        Raises:
            NotImplementedError: This method must be implemented by subclasses.
        """
        raise NotImplementedError

    @abstractmethod
    async def cancel_current_generation(self) -> None:
        """
        Cancel the current LLM generation if active.

        This abstract method must be implemented by subclasses to provide
        the ability to stop ongoing text generation. This is useful for
        implementing streaming controls and resource management and interruptions.

        Raises:
            NotImplementedError: This method must be implemented by subclasses.
        """
        # override in subclasses
        pass

    async def aclose(self) -> None:
        """
        Cleanup resources.

        This method ensures proper cleanup of LLM resources, including
        cancellation of any ongoing generation tasks. It should be called
        when the LLM instance is no longer needed.
        """
        await self.cancel_current_generation()
        pass

    async def __aenter__(self) -> LLM:
        """
        Async context manager entry point.

        Returns:
            LLM: The LLM instance for use in async context.
        """
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb) -> None:
        """
        Async context manager exit point.

        Ensures proper cleanup when exiting the async context by calling
        the aclose method.
        """
        await self.aclose()
